import os
import re
import sys
import pandas as pd
import urllib2
from bs4 import BeautifulSoup

# Source files which we process
source_file_name = "FY 16 - 17 PR Report Monthly Master.xlsx" # MOVE TO CONFIG
source_file = os.path.join(os.getcwd(),'..','..','data','external', source_file_name)

# Intermediate writes so we know whats going on, the data is not big and we overwrite here...
target_file_name = "processed_data.csv" # MOVE TO CONFIG
target_file = os.path.join(os.getcwd(),'..','..','data','processed', target_file_name)

columns_to_keep = ['DATE',
                   'CITY',
                   'STATE',
                   'COUNTRY',
                   'HEADLINE',
                   'IMPRESSIONS',
                   'PR MEDIA VALUE',
                   'PUB DATE',
                   'PUBLICATION',
                   'TYPE',
                   'URL',
                   'Jurisdictions Mentioned',
                   'Members Mentioned']

#Headers for urllib2
hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
       'Accept-Encoding': 'none',
       'Accept-Language': 'en-US,en;q=0.8',
       'Connection': 'keep-alive'}

def data_pull():
  """This method takes in the source_file that is generated by the Communications team. This file has a sheet for each month year 
   i.e. 'July 2016' and concatenates all the dataframes with a column for the month year. This data is then written to the 'data/processed'
   directory as a csv file and the returned data can be used for further processing"""

  data = pd.ExcelFile(source_file)
  sheets = data.sheet_names

  masterdata = pd.DataFrame()

  for sheet in sheets:
      tempdata = data.parse(sheet)
      tempdata['DATE'] = sheet
      masterdata = masterdata.append(tempdata)

  masterdata = masterdata[columns_to_keep]
  masterdata = masterdata.dropna(axis=0, how = 'any')
  #masterdata.to_csv(target_file, index=False)
  return masterdata
  
def process_url(url):
  """This method takes in a url and returns the text only from the page. All html and programming
  related characters should be removed before return"""
  query = urllib2.Request(url, None, hdr)
  html = urllib2.urlopen(query).read()
  soup = BeautifulSoup(html, "lxml")
  # kill all script and style elements
  for script in soup(["script", "style"]):
      script.extract()    # rip it out
  # get text
  text = soup.get_text(separator=' ')
  # break into lines and remove leading and trailing space on each
  lines = (line.strip() for line in text.splitlines())
  # break multi-headlines into a line each
  chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
  # drop blank lines
  text = '\n'.join(chunk for chunk in chunks if chunk)
  text = text.replace('\\', '')
  return text
